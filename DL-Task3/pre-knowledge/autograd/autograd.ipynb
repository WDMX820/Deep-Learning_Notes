{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b4873f83",
      "metadata": {
        "origin_pos": 0,
        "id": "b4873f83"
      },
      "source": [
        "# 自动微分\n",
        ":label:`sec_autograd`\n",
        "\n",
        "正如 :numref:`sec_calculus`中所说，求导是几乎所有深度学习优化算法的关键步骤。\n",
        "虽然求导的计算很简单，只需要一些基本的微积分。\n",
        "但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。\n",
        "\n",
        "深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。\n",
        "实际中，根据设计好的模型，系统会构建一个*计算图*（computational graph），\n",
        "来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n",
        "自动微分使系统能够随后反向传播梯度。\n",
        "这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
        "\n",
        "## 一个简单的例子\n",
        "\n",
        "作为一个演示例子，(**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**)。\n",
        "首先，我们创建变量`x`并为其分配一个初始值。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98cd8a9e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:31.627945Z",
          "iopub.status.busy": "2023-08-18T07:07:31.627424Z",
          "iopub.status.idle": "2023-08-18T07:07:32.686372Z",
          "shell.execute_reply": "2023-08-18T07:07:32.685559Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "98cd8a9e",
        "outputId": "2fd6d710-de00-4101-ee98-6769b65a9144"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.arange(4.0)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec430520",
      "metadata": {
        "origin_pos": 5,
        "id": "ec430520"
      },
      "source": [
        "[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度。**]\n",
        "重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
        "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
        "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27a5df4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.690633Z",
          "iopub.status.busy": "2023-08-18T07:07:32.689882Z",
          "iopub.status.idle": "2023-08-18T07:07:32.694159Z",
          "shell.execute_reply": "2023-08-18T07:07:32.693367Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "e27a5df4"
      },
      "outputs": [],
      "source": [
        "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
        "x.grad  # 默认值是None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd993524",
      "metadata": {
        "origin_pos": 10,
        "id": "bd993524"
      },
      "source": [
        "(**现在计算$y$。**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3f80b7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.698006Z",
          "iopub.status.busy": "2023-08-18T07:07:32.697167Z",
          "iopub.status.idle": "2023-08-18T07:07:32.705385Z",
          "shell.execute_reply": "2023-08-18T07:07:32.704593Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "4c3f80b7",
        "outputId": "7b3a7da5-d316-46a8-fe17-aa00dea5266d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35523dbc",
      "metadata": {
        "origin_pos": 15,
        "id": "35523dbc"
      },
      "source": [
        "`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。\n",
        "接下来，[**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**]，并打印这些梯度。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c3a419",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.708698Z",
          "iopub.status.busy": "2023-08-18T07:07:32.708196Z",
          "iopub.status.idle": "2023-08-18T07:07:32.713924Z",
          "shell.execute_reply": "2023-08-18T07:07:32.713091Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "a1c3a419",
        "outputId": "78785b4c-144b-4bee-ccda-eeb0a0d87db4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.backward()  #通过调用反向传播函数来自动计算y关于x每个分量的梯度\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dca6a271",
      "metadata": {
        "origin_pos": 20,
        "id": "dca6a271"
      },
      "source": [
        "函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n",
        "让我们快速验证这个梯度是否计算正确。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8493d0a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.718858Z",
          "iopub.status.busy": "2023-08-18T07:07:32.718156Z",
          "iopub.status.idle": "2023-08-18T07:07:32.724091Z",
          "shell.execute_reply": "2023-08-18T07:07:32.723104Z"
        },
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "id": "b8493d0a",
        "outputId": "3d320896-fb08-4f44-952e-5e6a4995b9ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad == 4 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2733c623",
      "metadata": {
        "origin_pos": 25,
        "id": "2733c623"
      },
      "source": [
        "[**现在计算`x`的另一个函数。**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2fcd392",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.729368Z",
          "iopub.status.busy": "2023-08-18T07:07:32.728433Z",
          "iopub.status.idle": "2023-08-18T07:07:32.736493Z",
          "shell.execute_reply": "2023-08-18T07:07:32.735715Z"
        },
        "origin_pos": 27,
        "tab": [
          "pytorch"
        ],
        "id": "f2fcd392",
        "outputId": "8c9d2a45-6f61-4827-821c-c2d1f5564d98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
        "x.grad.zero_()  #将梯度归零\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f4f459",
      "metadata": {
        "origin_pos": 30,
        "id": "58f4f459"
      },
      "source": [
        "## 非标量变量的反向传播\n",
        "\n",
        "当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。\n",
        "对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n",
        "\n",
        "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），\n",
        "但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。\n",
        "这里(**，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4e62a5d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.740109Z",
          "iopub.status.busy": "2023-08-18T07:07:32.739419Z",
          "iopub.status.idle": "2023-08-18T07:07:32.745803Z",
          "shell.execute_reply": "2023-08-18T07:07:32.744893Z"
        },
        "origin_pos": 32,
        "tab": [
          "pytorch"
        ],
        "id": "f4e62a5d",
        "outputId": "73f4206a-6848-4c8d-e063-8ad95c8a7c09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
        "# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\n",
        "x.grad.zero_()\n",
        "y = x * x\n",
        "# 等价于y.backward(torch.ones(len(x)))\n",
        "y.sum().backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80f510c4",
      "metadata": {
        "origin_pos": 35,
        "id": "80f510c4"
      },
      "source": [
        "## 分离计算\n",
        "\n",
        "有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n",
        "例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n",
        "想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数，\n",
        "并且只考虑到`x`在`y`被计算后发挥的作用。\n",
        "\n",
        "这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，\n",
        "但丢弃计算图中如何计算`y`的任何信息。\n",
        "换句话说，梯度不会向后流经`u`到`x`。\n",
        "因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，\n",
        "而不是`z=x*x*x`关于`x`的偏导数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dab493d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.749398Z",
          "iopub.status.busy": "2023-08-18T07:07:32.748759Z",
          "iopub.status.idle": "2023-08-18T07:07:32.755280Z",
          "shell.execute_reply": "2023-08-18T07:07:32.754543Z"
        },
        "origin_pos": 37,
        "tab": [
          "pytorch"
        ],
        "id": "8dab493d",
        "outputId": "9e753aeb-cc5d-4a61-bc62-9a86df873e02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "u = y.detach()  #将某些计算移动到记录的计算图之外\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8fe6f9c",
      "metadata": {
        "origin_pos": 40,
        "id": "f8fe6f9c"
      },
      "source": [
        "由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，\n",
        "得到`y=x*x`关于的`x`的导数，即`2*x`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271a9b3a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.759344Z",
          "iopub.status.busy": "2023-08-18T07:07:32.758633Z",
          "iopub.status.idle": "2023-08-18T07:07:32.764663Z",
          "shell.execute_reply": "2023-08-18T07:07:32.763922Z"
        },
        "origin_pos": 42,
        "tab": [
          "pytorch"
        ],
        "id": "271a9b3a",
        "outputId": "0def12d3-94ea-4e89-a7b2-ae373913bdd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd79d12f",
      "metadata": {
        "origin_pos": 45,
        "id": "fd79d12f"
      },
      "source": [
        "## Python控制流的梯度计算\n",
        "\n",
        "使用自动微分的一个好处是：\n",
        "[**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。\n",
        "在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6323b2ff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.769249Z",
          "iopub.status.busy": "2023-08-18T07:07:32.768616Z",
          "iopub.status.idle": "2023-08-18T07:07:32.773175Z",
          "shell.execute_reply": "2023-08-18T07:07:32.772293Z"
        },
        "origin_pos": 47,
        "tab": [
          "pytorch"
        ],
        "id": "6323b2ff"
      },
      "outputs": [],
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51aaf333",
      "metadata": {
        "origin_pos": 50,
        "id": "51aaf333"
      },
      "source": [
        "让我们计算梯度。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7719d6b6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.777740Z",
          "iopub.status.busy": "2023-08-18T07:07:32.777207Z",
          "iopub.status.idle": "2023-08-18T07:07:32.782254Z",
          "shell.execute_reply": "2023-08-18T07:07:32.781458Z"
        },
        "origin_pos": 52,
        "tab": [
          "pytorch"
        ],
        "id": "7719d6b6"
      },
      "outputs": [],
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()  #即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "816a1ac2",
      "metadata": {
        "origin_pos": 55,
        "id": "816a1ac2"
      },
      "source": [
        "我们现在可以分析上面定义的`f`函数。\n",
        "请注意，它在其输入`a`中是分段线性的。\n",
        "换言之，对于任何`a`，存在某个常量标量`k`，使得`f(a)=k*a`，其中`k`的值取决于输入`a`，因此可以用`d/a`验证梯度是否正确。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2595bdc0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.785728Z",
          "iopub.status.busy": "2023-08-18T07:07:32.785179Z",
          "iopub.status.idle": "2023-08-18T07:07:32.790672Z",
          "shell.execute_reply": "2023-08-18T07:07:32.789892Z"
        },
        "origin_pos": 57,
        "tab": [
          "pytorch"
        ],
        "id": "2595bdc0",
        "outputId": "55de8d4b-5ef3-4279-fcfe-d81707aeec1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.grad == d / a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67fb5517",
      "metadata": {
        "origin_pos": 60,
        "id": "67fb5517"
      },
      "source": [
        "## 小结\n",
        "\n",
        "* 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
        "\n",
        "## 练习\n",
        "\n",
        "1. 为什么计算二阶导数比一阶导数的开销要更大？\n",
        "1. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
        "1. 在控制流的例子中，我们计算`d`关于`a`的导数，如果将变量`a`更改为随机向量或矩阵，会发生什么？\n",
        "1. 重新设计一个求控制流梯度的例子，运行并分析结果。\n",
        "1. 使$f(x)=\\sin(x)$，绘制$f(x)$和$\\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\\cos(x)$。\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.为什么计算二阶导数比一阶导数的开销要更大？\n",
        "计算二阶导数（Hessian）比计算一阶导数（梯度）的开销大主要有以下几个原因：\n",
        " 复杂性：计算二阶导数涉及到对梯度向量中的每个分量进行偏导数的计算。这意味着在 n 维空间中，计算 Hessian 需要 n^2 个偏导数，而计算梯度只需要 n 个偏导数。\n",
        " 存储和计算成本：Hessian 矩阵通常是一个 n×n 的矩阵，而梯度只需一个 n 维向量。这在内存和计算时间上都会增加开销。\n",
        " 数值稳定性：在计算二阶导数时，数值稳定性可能会受到挑战，特别是在使用数值方法进行逼近时，可能会导致额外的误差。\n",
        "\n",
        "2.反向传播函数的重复运行会发生什么？\n",
        "在深度学习框架（如 TensorFlow 或 PyTorch）中，当调用反向传播函数后，梯度会被计算并存储（通常是通过链式法则）。如果立即再次运行反向传播函数：\n",
        " 梯度会累加：在许多框架中，如果不清除梯度，梯度会在每次反向传播时累加。这样可能导致最终梯度的计算不是我们想要的结果。\n",
        " 需要重置：通常需要在每次反向传播之前调用 zero_grad()，以清除先前的梯度。\n",
        "\n",
        "3.控制流与随机变量或矩阵的导数\n",
        "如果您计算某个函数 d 关于一个变量 a 的导数，但将 a 更改为一个随机向量或矩阵，可能会发生以下情况：\n",
        " 误差传播：如果控制流或分支条件依赖于 a 的值，随机化可能会导致程序执行的分支变化，从而使梯度的计算无意义。\n",
        " 维度不匹配：随机向量或矩阵的使用可能会导致维度不一致，特别是在计算梯度或 Hessian 时。\n",
        "\n",
        "4.重新设计控制流梯度的示例\n",
        " 下面是一个简单的示例，其中我们计算函数 f(a)=a^2 的导数，使用控制流来处理条件：\n",
        "'''\n",
        "import torch\n",
        "# 控制流示例\n",
        "def control_flow_example(a):\n",
        "    if a.item() > 0:\n",
        "        return a ** 2\n",
        "    else:\n",
        "        return -a ** 2\n",
        "\n",
        "# 随机向量\n",
        "a = torch.tensor(1.0, requires_grad=True)\n",
        "output = control_flow_example(a)\n",
        "output.backward()\n",
        "print(\"Gradient with a = 1.0:\", a.grad)\n",
        "\n",
        "a = torch.tensor(-1.0, requires_grad=True)\n",
        "output = control_flow_example(a)\n",
        "output.backward()\n",
        "print(\"Gradient with a = -1.0:\", a.grad)\n",
        "\n",
        "'''\n",
        "5. 画出 f(x)=sin⁡(x) 的图像以及其导数\n",
        "我们将在不使用已知导数的方法 f'(x)=cos⁡(x) 的情况下，通过数值方法计算并绘制。\n",
        "以下是 Python 代码：\n",
        "'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 定义函数\n",
        "def f(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "# 数值计算导数\n",
        "def numerical_derivative(f, x, h=1e-5):\n",
        "    return (f(x + h) - f(x - h)) / (2 * h)\n",
        "\n",
        "# 定义 x 的取值范围\n",
        "x_values = np.linspace(-2 * np.pi, 2 * np.pi, 100)\n",
        "f_values = f(x_values)\n",
        "df_values = numerical_derivative(f, x_values)\n",
        "\n",
        "# 绘制函数和导数\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(x_values, f_values, label='f(x) = sin(x)', color='blue')\n",
        "plt.plot(x_values, df_values, label=\"Numerical df(x)/dx\", linestyle='--', color='orange')\n",
        "plt.title('Function f(x) and its Numerical Derivative')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.axhline(0, color='black', linewidth=0.5, ls='--')\n",
        "plt.axvline(0, color='black', linewidth=0.5, ls='--')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "总结\n",
        "计算开销：二阶导数计算比一阶导数更复杂且开销更大。\n",
        "反向传播：多次运行期间梯度可累加，需注意重置。\n",
        "随机变量：修改变量可能导致梯度计算出错。\n",
        "控制流：控制流下的导数计算可以具有挑战性，变化值会影响结果。\n",
        "数值导数：通过差分法计算导数，而不仅依赖解析解。\n",
        "'''"
      ],
      "metadata": {
        "id": "LkTz0mWUJviG"
      },
      "id": "LkTz0mWUJviG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "530f74f8",
      "metadata": {
        "origin_pos": 62,
        "tab": [
          "pytorch"
        ],
        "id": "530f74f8"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1759)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}